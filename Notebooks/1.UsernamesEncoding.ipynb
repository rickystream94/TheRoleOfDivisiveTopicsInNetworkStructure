{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilities**:\n",
    "- Windows keystroke for \\` character is ALT+96\n",
    "- Windows keystroke for ~ character is ALT+126\n",
    "\n",
    "**Notebook Initialization**: Hereby all the needed `import` statements, global variables or functions with global scope throughout the notebook.\n",
    "**PySpark** has been used for data processing to avoid overloading memory usage and take advantage of the benefits from the **RDD** (Resilient Distributed Dataset) format to deal with huge files without the explicit need of loading all their contents into memory.\n",
    "\n",
    "Run the cell below once to initialize all the needed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "%load_ext Cython\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize PySpark\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Global Settings\n",
    "dataset_dir = \"../data/mmr_graph/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "Storing Twitter usernames without any transformation in memory would mean huge memory consumption to store a relatively expensive data type such as strings. I could optimize this by performing some *data encoding* on the usernames and convert strings to integers by keeping a 1:1 mapping between the string representation of the username and its integer representation. This would result in huge memory-usage optimization when loading the graph into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Twitter Usernames Encoding\n",
    "Since I am dealing with huge files (19GB overall for the MMR dataset) I have to pay attention with what is actually loaded into memory, although the current server configuration offers 128GB available RAM. As a result of the encoding process, I want to create a .CSV file that will store the 1:1 mapping between the (string) Twitter username and its integer encoding. As a rule of thumb, it's just easy to start counting from 0 and assign a unique ID to each username. Output is saved to a `usernames.csv` file. I also wanted to create an encoded copy of the MMR dataset in the form of a single .CSV file, which would then be also easier to parse. It's also just convenient to have a single dataset file as a result of the encoding process, named `mmr_encoded.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "usernames_filename = \"../data/usernames.csv\"\n",
    "usernames_header_filename = \"../data/usernames_header.csv\"\n",
    "mmr_encoded_filename = \"../data/mmr_encoded.csv\"\n",
    "csv_header = [\"username\", \"encoding:ID(User)\"]\n",
    "\n",
    "# Support functions declarations\n",
    "def add_or_get_new_user_encoding(username):\n",
    "    global current_user_id\n",
    "    global usernames_encoding\n",
    "    if not username in usernames_encoding.keys():\n",
    "        user_enc = current_user_id\n",
    "        usernames_encoding[username] = user_enc\n",
    "        current_user_id += 1\n",
    "        if current_user_id%10000000 == 0:\n",
    "            print(\"Currently processed {0} unique usernames\".format(current_user_id))\n",
    "        return user_enc\n",
    "    return usernames_encoding[username]\n",
    "\n",
    "def get_encoding(username):\n",
    "    usernames_rdd = sc.textFile(usernames_filename)\n",
    "    return usernames_rdd.map(lambda x: x.split(',')).filter(lambda x: x[0] == username).map(lambda x: int(x[1])).first()\n",
    "\n",
    "def process_line(line):\n",
    "    a,b = line.strip(\"()\\n\").split(', ')\n",
    "    a = a[2:-1]\n",
    "    b = b[2:-1]\n",
    "    a_enc = add_or_get_new_user_encoding(a)\n",
    "    b_enc = add_or_get_new_user_encoding(b)\n",
    "    return a_enc, b_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicating dataset in encoded format...\n",
      "Processing files in folder 2013-09...\n",
      "Currently processed 10000000 unique usernames\n",
      "Currently processed 20000000 unique usernames\n",
      "Processing files in folder 2013-12...\n",
      "Currently processed 30000000 unique usernames\n",
      "Processing files in folder 2014-03...\n",
      "Currently processed 40000000 unique usernames\n",
      "Processing files in folder 2014-06...\n",
      "Currently processed 50000000 unique usernames\n",
      "Processing files in folder 2014-09...\n",
      "Processing files in folder 2014-12...\n",
      "Processing files in folder 2015-03...\n",
      "Currently processed 60000000 unique usernames\n",
      "Processing files in folder 2015-06...\n",
      "Processing files in folder 2015-09...\n",
      "Currently processed 70000000 unique usernames\n",
      "Processing files in folder 2015-12...\n",
      "Processing files in folder 2016-03...\n",
      "Currently processed 80000000 unique usernames\n",
      "Processing files in folder 2016-06...\n",
      "Processing files in folder 2016-09...\n",
      "Writing CSV header to separate file ../data/usernames_header.csv...\n",
      "Dumping encoded usernames to ../data/usernames.csv...\n",
      "Processed all usernames. Total unique usernames: 89577277\n",
      "CPU times: user 38min 21s, sys: 30.9 s, total: 38min 52s\n",
      "Wall time: 39min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def start_encoding():\n",
    "    print(\"Duplicating dataset in encoded format...\")\n",
    "    with open(mmr_encoded_filename, \"w\") as mmr_enc_f:\n",
    "        for folder in sorted(os.listdir(dataset_dir)):\n",
    "            folder_path = os.path.join(dataset_dir, folder)\n",
    "            print(\"Processing files in folder {0}...\".format(folder))\n",
    "            for part in os.listdir(folder_path):\n",
    "                part_path = os.path.join(folder_path, part)\n",
    "                if os.path.isfile(part_path) and part.startswith(\"part\"):\n",
    "                    with open(part_path, encoding=\"utf-8\") as part:\n",
    "                        for line in part:\n",
    "                            a_enc, b_enc = process_line(line)\n",
    "                            mmr_enc_f.write(\",\".join([str(a_enc),str(b_enc)]) + \"\\n\")\n",
    "\n",
    "    print(\"Writing CSV header to separate file {0}...\".format(usernames_header_filename))\n",
    "    with open(usernames_header_filename, \"w\") as usernames_header_f:\n",
    "        usernames_header_f.write(\",\".join(csv_header)+\"\\n\")\n",
    "    print(\"Dumping encoded usernames to {0}...\".format(usernames_filename))\n",
    "    with open(usernames_filename, \"w\") as usernames_out_f:\n",
    "        for k in usernames_encoding.keys():\n",
    "            usernames_out_f.write(\",\".join([k, str(usernames_encoding[k])])+\"\\n\")\n",
    "    print(\"Processed all usernames. Total unique usernames: {0}\".format(current_user_id))\n",
    "\n",
    "# Start processing\n",
    "current_user_id = 0 #Start with ID = 0\n",
    "usernames_encoding = {}\n",
    "\n",
    "# If any of the expected output files are missing, run the cell block.\n",
    "if not os.path.exists(usernames_filename) or not os.path.exists(mmr_encoded_filename):\n",
    "    start_encoding()\n",
    "    del usernames_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the encoding process outlined above, these are the two output files and their respective size that will be used from now on:\n",
    "\n",
    "| `usernames.csv` | `mmr_encoded.csv`   |\n",
    "|---|---|\n",
    "|   1.8GB         |                9.8GB|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Querying for Username-Encoding\n",
    "As a reference and last step of this Jupyter Notebook, I include how one can, given a username/encoding, efficiently query for its corresponding encoding/username by using **PySpark RDDs and pure bash scripting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Examples: Query for encoding, given the username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 12 ms, total: 24 ms\n",
      "Wall time: 8.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4969601"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "get_encoding(\"barackobama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 7.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3793089"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "get_encoding(\"realdonaldtrump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Examples: Query for username, given the encoding\n",
    "Given a username encoding, querying for its corresponding string representation is trivial with the following **bash script**:\n",
    "\n",
    "~~~bash\n",
    "USERNAMES=\"../data/usernames.csv\"\n",
    "ENCODING=$1\n",
    "LINE_NUMBER=$(($ENCODING+1))\n",
    "\n",
    "sed -n \"$LINE_NUMBER\"p $USERNAMES\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ryofujii0311,46915512\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "../scripts/get_username.sh 46915512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohigeforever,49309989\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "../scripts/get_username.sh 49309989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Super-inefficient example using Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 49s ± 398 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "CPU times: user 292 ms, sys: 56 ms, total: 348 ms\n",
      "Wall time: 34min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using a DataFrame\n",
    "usernames_rdd = sc.textFile(usernames_filename)\n",
    "df_fields = [StructField(csv_header[0], StringType(), False), StructField(csv_header[1], IntegerType(), False)]\n",
    "df_schema = StructType(df_fields)\n",
    "header_rdd = usernames_rdd.filter(lambda l: \",\".join(csv_header) in l)\n",
    "usernames_noHeader_rdd = usernames_rdd.subtract(header_rdd)\n",
    "usernames_df = usernames_noHeader_rdd.map(lambda l: l.split(',')).map(lambda p: (p[0], int(p[1]))).toDF(df_schema)\n",
    "%timeit usernames_df.where(usernames_df.username == \"barackobama\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats above probably show that PySpark dataframe is much slower than the RDD version, therefore for the easy purpose as retrieving the username / encoding of a given input we can just safely rely on the first option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
