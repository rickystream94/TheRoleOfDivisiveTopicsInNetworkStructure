{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline Analysis\n",
    "Scope of the following analysis is to investigate whether the previously analyzed hashtags actually play a significant role in the underlying network structure: does a specific topic (hashtag) strengthen ties or increases diversity within Twitter users? The idea is therefore to build a metric and extract this information by combining mutual interactions Tweets and the network structure that represented the sole core of the analysis so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snap\n",
    "from snap import TUNGraph\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import seaborn as sns\n",
    "from __future__ import print_function\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "# Set Seaborn defaults\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.precision\", 6)\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "mpl.rcParams['savefig.dpi'] = 100\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "# Global variables\n",
    "data_dir = \"../data\"\n",
    "pictures_path = os.path.join(\"../Pictures\", \"8.TimelineAnalysis\")\n",
    "tweets_path = \"../lib/GetOldTweets-python/out/completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Creation of Model Classes\n",
    "First of all, I might need to be a bit more concerned about performance since some Tweets files are pretty big (a few GBs), therefore I'd like to optimize some operations as much as possible.\n",
    "- Since Tweets come naturally with a unique ID, I may create a `Tweet` custom type;\n",
    "- Class `Interaction` representing an interaction between 2 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, tweet_id, users, tweet_dict):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.tweet_dict = tweet_dict\n",
    "        self.users = users\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Tweet):\n",
    "            return self.tweet_id == other.tweet_id\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        x = self.__eq__(other)\n",
    "        if x is not NotImplemented:\n",
    "            return not x\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:        \n",
    "    def __init__(self, source, target, periods=None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.periods = periods\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Interaction):\n",
    "            return (self.source == other.source and self.target == other.target) or (self.source == other.target and self.target == other.source)\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        x = self.__eq__(other)\n",
    "        if x is not NotImplemented:\n",
    "            return not x\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(hash(self.source)+hash(self.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby a collection of some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_percentage(n,m):\n",
    "    return n*100.0/m\n",
    "\n",
    "def load_graph_from_backup(filename):\n",
    "    FIn = snap.TFIn(\"../data/\"+filename+\".bin\")\n",
    "    graph = snap.TUNGraph.Load(FIn)\n",
    "    return graph\n",
    "\n",
    "def read_large_file(file_object):\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data.rstrip('\\n')\n",
    "        \n",
    "def process_edge_line(line):\n",
    "    source, target, prop = line.split(',')\n",
    "    return int(source), int(target), [int(p) for p in prop.split(';')]\n",
    "        \n",
    "def get_usernames_from_basic_tweet_info(hashtag, tweet):\n",
    "    usernames = set()\n",
    "    # (1): Has tweeted using hashtag\n",
    "    if hashtag in [h.lower() for h in tweet[\"entities\"][\"hashtags\"]]:\n",
    "        usernames.add(tweet[\"user\"][\"screen_name\"].lower())\n",
    "\n",
    "    # (2): Has been mentioned / replied to\n",
    "    if not tweet[\"in_reply_to_screen_name\"] is None:\n",
    "        usernames.add(tweet[\"in_reply_to_screen_name\"].lower())\n",
    "    for mentions in tweet[\"entities\"][\"user_mentions\"]:\n",
    "        usernames.add(mentions[\"screen_name\"].lower())\n",
    "    return usernames\n",
    "\n",
    "def get_tweet_usernames(hashtag, tweet):\n",
    "    usernames = set()\n",
    "    usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet))\n",
    "    if \"retweeted_status\" in tweet:\n",
    "        usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet[\"retweeted_status\"]))\n",
    "    if \"quoted_status\" in tweet:\n",
    "        usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet[\"quoted_status\"]))\n",
    "    return usernames\n",
    "\n",
    "def extract_hashtag_usernames(hashtag, tweets):\n",
    "    hashtag_usernames = set()\n",
    "    for tweet in tweets:\n",
    "        hashtag_usernames.update(tweet.users)\n",
    "    print(\"Total unique usernames involved in '#%s' hashtag conversations from %d tweets: %d\" %(hashtag, len(tweets), len(hashtag_usernames)))\n",
    "    return hashtag_usernames\n",
    "\n",
    "# Extract tweets given a specific hashtag\n",
    "def get_tweets(hashtag):\n",
    "    tweets_filename = os.path.join(tweets_path,\"tweets_#\" + hashtag + \"_2013-09-01_2016-12-31.json\")\n",
    "    tweets = set()\n",
    "    with open(tweets_filename) as fin:\n",
    "        for line in read_large_file(fin):\n",
    "            tweet_dict = json.loads(line)\n",
    "            tweet_id = np.int64(tweet_dict[\"id_str\"])\n",
    "            tweet_users = get_tweet_usernames(hashtag, tweet_dict)\n",
    "            tweets.add(Tweet(tweet_id, tweet_users, tweet_dict))\n",
    "    print(\"Imported %d tweets from %s\" %(len(tweets),tweets_filename))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Tweets: filter usernames and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"makeamericagreatagain\"\n",
    "hashtag_full = \"#MAGA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hashtag subgraph from backup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_subgraph = load_graph_from_backup(\"mmr_subgraph_\"+hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1498400 tweets from ../lib/GetOldTweets-python/out/completed/tweets_#makeamericagreatagain_2013-09-01_2016-12-31.json\n",
      "CPU times: user 1min 17s, sys: 4.47 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets = get_tweets(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique usernames involved in '#makeamericagreatagain' hashtag conversations from 1498400 tweets: 346194\n"
     ]
    }
   ],
   "source": [
    "hashtag_usernames = extract_hashtag_usernames(hashtag, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 708 ms, total: 2min 52s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "usernames_to_id_dict = {}\n",
    "with open(\"../data/usernames.csv\") as usernames_f:\n",
    "    for line in read_large_file(usernames_f):\n",
    "        username = line.split(',')[0]\n",
    "        encoding = int(line.split(',')[1])\n",
    "        # Add to dict only if username is part of the hashtag subgraph\n",
    "        if hashtag_subgraph.IsNode(encoding):\n",
    "            usernames_to_id_dict[username] = encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is then to filter out those Tweets whose involved users are not part of the corresponding $H$ subgraph (i.e. if none of its involved users represents a node in $H$). The keys of the `usernames_to_id_dict` dictionary correspond to all the usernames of the $H$ subgraph, so it should be sufficient to check the following criteria:\n",
    "- A tweet is kept if any of its involved users is part of the $H$ subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered tweets (with at least 1 involved user within MMR graph data): 1192392 (79.58% of 1498400 total tweets)\n"
     ]
    }
   ],
   "source": [
    "tweets_filtered = filter(lambda t: any(map(lambda u: u in usernames_to_id_dict, t.users)), tweets)\n",
    "print(\"Number of filtered tweets (with at least 1 involved user within MMR graph data): %d (%.2f%% of %d total tweets)\" %(len(tweets_filtered), get_relative_percentage(len(tweets_filtered), len(tweets)), len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above: among 1.498.400 total tweets, **1.192.392** involve at least one user that is part of the MMR graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users involved in 1192392 filtered tweets: 279568 (80.75% of 346194 total usernames)\n"
     ]
    }
   ],
   "source": [
    "def count_tot_users(tweets):\n",
    "    tot_users = set()\n",
    "    for t in tweets:\n",
    "        tot_users.update(t.users)\n",
    "    return len(tot_users)\n",
    "\n",
    "tot_users = count_tot_users(tweets_filtered)\n",
    "print(\"Total number of unique users involved in %d filtered tweets: %d (%.2f%% of %d total usernames)\" %(len(tweets_filtered), tot_users, get_relative_percentage(tot_users, len(hashtag_usernames)), len(hashtag_usernames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Of course the number of users involved in the filtered tweets may be higher than the actual number of nodes in $H$, since each Tweet might include users that have not been captured by the MMR data. The reason why I decided to apply this filtering step, is that the users collected by the MMR graph data hide some extra interaction properties, e.g. 2 users are related if their total number of interactions within a 3 months period is sufficiently high, so that it represents a constant interaction over time and not a random one. This way we would already remove a lot of noisy/irrelevant data and speed up the algorithms in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby a summary of the filtering steps and the statistics about the collected tweets and usernames:\n",
    "\n",
    "| Property | Before Filtering | After Filtering | % After/Before\n",
    "|---|---|---|---|\n",
    "| **Total Collected Tweets** | 1.498.400 | 1.192.392 | 79,58%\n",
    "| **Total Unique Usernames** | 346.194 | 279.568 | 80,75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. User Interactions Statistics\n",
    "I could first show some statistics about the interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Top Interactions\n",
    "I may show which are the interactions that occurred the most throughout the years we're considering. It's convenient to create a dictionary with $K \\rightarrow V$ pairs where $K$ = interaction, $V$ = count (count how many times two people have interacted with each other within the whole timeline period we're considering).\n",
    "\n",
    "Collecting the interactions count dictionary may be done linearly with a single pass, and the steps are hereby summarized:\n",
    "1. For each tweet, create a list of all possible pairs of its involved users and for each of them create an `Interaction` instance;\n",
    "2. Add a new entry to the dictionary with value 1, if the interaction is not existing;\n",
    "3. If the interaction is already existing, increment its value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions_count_dict(tweets):\n",
    "    interactions_count = {}\n",
    "    for t in tweets:\n",
    "        pairs = list(combinations(t.users, 2))\n",
    "        for p in pairs:\n",
    "            i = Interaction(p[0], p[1])\n",
    "            if i in interactions_count:\n",
    "                interactions_count[i] += 1\n",
    "            else:\n",
    "                interactions_count[i] = 1\n",
    "    return interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_count = create_interactions_count_dict(tweets_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results conveniently through a DataFrame, by first sorting it with descending counts (I only show the top interactions e.g. whose count value are at least 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(data=[(el[0].source, el[0].target, el[1]) for el in interactions_count.items()], columns=[\"Source\", \"Target\", \"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319136</th>\n",
       "      <td>danscavino</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>16937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119519</th>\n",
       "      <td>foxnews</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744096</th>\n",
       "      <td>hillaryclinton</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>11425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752093</th>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62617</th>\n",
       "      <td>ilduce2016</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314171</th>\n",
       "      <td>cnn</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>globalsocialm2</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>6264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402991</th>\n",
       "      <td>gop</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>5730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429765</th>\n",
       "      <td>erictrump</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467220</th>\n",
       "      <td>seanhannity</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>4959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306240</th>\n",
       "      <td>mike_pence</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849942</th>\n",
       "      <td>donaldjtrumpjr</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>4136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848269</th>\n",
       "      <td>marcorubio</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374129</th>\n",
       "      <td>megynkelly</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>jebbush</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821322</th>\n",
       "      <td>clewandowski_</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776981</th>\n",
       "      <td>teamtrump</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880432</th>\n",
       "      <td>djtftw</td>\n",
       "      <td>infowars</td>\n",
       "      <td>2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789836</th>\n",
       "      <td>cnn</td>\n",
       "      <td>foxnews</td>\n",
       "      <td>2543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428147</th>\n",
       "      <td>michaelcohen212</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517023</th>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>ivankatrump</td>\n",
       "      <td>2419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342566</th>\n",
       "      <td>uonguard</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555457</th>\n",
       "      <td>redeemed1952</td>\n",
       "      <td>patriotmike1</td>\n",
       "      <td>2249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376887</th>\n",
       "      <td>trump_world</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752143</th>\n",
       "      <td>donaldjtrumpjr</td>\n",
       "      <td>erictrump</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666503</th>\n",
       "      <td>o_irisht</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8985</th>\n",
       "      <td>anncoulter</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692867</th>\n",
       "      <td>realbencarson</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350516</th>\n",
       "      <td>potus</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323748</th>\n",
       "      <td>hillx123</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596931</th>\n",
       "      <td>lindasuhler</td>\n",
       "      <td>o_irisht</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480130</th>\n",
       "      <td>kennyscott1952</td>\n",
       "      <td>woapradio</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544029</th>\n",
       "      <td>judymichiganmom</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492716</th>\n",
       "      <td>tryceankrom</td>\n",
       "      <td>teamcavuto</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798963</th>\n",
       "      <td>alcardfan</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568032</th>\n",
       "      <td>usatrusttrump</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339122</th>\n",
       "      <td>renee052270</td>\n",
       "      <td>ehmart</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134804</th>\n",
       "      <td>jonjburrows</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425246</th>\n",
       "      <td>surfphx</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559420</th>\n",
       "      <td>denise247365</td>\n",
       "      <td>ehmart</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523495</th>\n",
       "      <td>teamtrump</td>\n",
       "      <td>berniesanders</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146468</th>\n",
       "      <td>stephenfhayes</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197898</th>\n",
       "      <td>surgrn123</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117055</th>\n",
       "      <td>harmony_nation</td>\n",
       "      <td>clewandowski_</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37757</th>\n",
       "      <td>dloesch</td>\n",
       "      <td>glennbeck</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185333</th>\n",
       "      <td>missamericapie</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82740</th>\n",
       "      <td>clewandowski_</td>\n",
       "      <td>bbretonwindham</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458987</th>\n",
       "      <td>joyreaper</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27696</th>\n",
       "      <td>pink_lady56</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362434</th>\n",
       "      <td>morning_joe</td>\n",
       "      <td>l8deis</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242315</th>\n",
       "      <td>spikemaster8</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377525</th>\n",
       "      <td>johnjaeger_mt</td>\n",
       "      <td>foxnews</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449103</th>\n",
       "      <td>brithume</td>\n",
       "      <td>megynkelly</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57701</th>\n",
       "      <td>americanmex067</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322217</th>\n",
       "      <td>kimguilfoyle</td>\n",
       "      <td>ericbolling</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537574</th>\n",
       "      <td>miriamrosemc</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442774</th>\n",
       "      <td>laraleatrump</td>\n",
       "      <td>ivankatrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593725</th>\n",
       "      <td>debalwaystrump</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702867</th>\n",
       "      <td>carrieksada</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689072</th>\n",
       "      <td>danscavino</td>\n",
       "      <td>paulmanafort</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1486 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Source           Target  Count\n",
       "319136       danscavino  realdonaldtrump  16937\n",
       "119519          foxnews  realdonaldtrump  12841\n",
       "744096   hillaryclinton  realdonaldtrump  11425\n",
       "752093  realdonaldtrump          tedcruz   8585\n",
       "62617        ilduce2016  realdonaldtrump   7234\n",
       "314171              cnn  realdonaldtrump   7197\n",
       "1948     globalsocialm2  realdonaldtrump   6264\n",
       "402991              gop  realdonaldtrump   5730\n",
       "429765        erictrump  realdonaldtrump   5101\n",
       "467220      seanhannity  realdonaldtrump   4959\n",
       "306240       mike_pence  realdonaldtrump   4413\n",
       "849942   donaldjtrumpjr  realdonaldtrump   4136\n",
       "848269       marcorubio  realdonaldtrump   3000\n",
       "374129       megynkelly  realdonaldtrump   2992\n",
       "64146           jebbush  realdonaldtrump   2771\n",
       "821322    clewandowski_  realdonaldtrump   2706\n",
       "776981        teamtrump  realdonaldtrump   2617\n",
       "880432           djtftw         infowars   2590\n",
       "789836              cnn          foxnews   2543\n",
       "428147  michaelcohen212  realdonaldtrump   2527\n",
       "517023  realdonaldtrump      ivankatrump   2419\n",
       "342566         uonguard  realdonaldtrump   2387\n",
       "555457     redeemed1952     patriotmike1   2249\n",
       "376887      trump_world  realdonaldtrump   2218\n",
       "752143   donaldjtrumpjr        erictrump   2200\n",
       "666503         o_irisht  realdonaldtrump   2130\n",
       "8985         anncoulter  realdonaldtrump   2057\n",
       "692867    realbencarson  realdonaldtrump   2048\n",
       "350516            potus  realdonaldtrump   2043\n",
       "323748         hillx123  realdonaldtrump   1981\n",
       "...                 ...              ...    ...\n",
       "596931      lindasuhler         o_irisht    102\n",
       "480130   kennyscott1952        woapradio    102\n",
       "544029  judymichiganmom  realdonaldtrump    102\n",
       "492716      tryceankrom       teamcavuto    102\n",
       "798963        alcardfan  realdonaldtrump    102\n",
       "568032    usatrusttrump  realdonaldtrump    101\n",
       "339122      renee052270           ehmart    101\n",
       "134804      jonjburrows  realdonaldtrump    101\n",
       "425246          surfphx  realdonaldtrump    101\n",
       "559420     denise247365           ehmart    101\n",
       "523495        teamtrump    berniesanders    101\n",
       "146468    stephenfhayes  realdonaldtrump    101\n",
       "197898        surgrn123  realdonaldtrump    101\n",
       "117055   harmony_nation    clewandowski_    101\n",
       "37757           dloesch        glennbeck    101\n",
       "185333   missamericapie  realdonaldtrump    101\n",
       "82740     clewandowski_   bbretonwindham    101\n",
       "458987        joyreaper  realdonaldtrump    101\n",
       "27696       pink_lady56  realdonaldtrump    101\n",
       "362434      morning_joe           l8deis    100\n",
       "242315     spikemaster8  realdonaldtrump    100\n",
       "377525    johnjaeger_mt          foxnews    100\n",
       "449103         brithume       megynkelly    100\n",
       "57701    americanmex067  realdonaldtrump    100\n",
       "322217     kimguilfoyle      ericbolling    100\n",
       "537574     miriamrosemc  realdonaldtrump    100\n",
       "442774     laraleatrump      ivankatrump    100\n",
       "593725   debalwaystrump  realdonaldtrump    100\n",
       "702867      carrieksada  realdonaldtrump    100\n",
       "689072       danscavino     paulmanafort    100\n",
       "\n",
       "[1486 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.sort_values(by=\"Count\", ascending=False, inplace=True)\n",
    "interactions_df[interactions_df[\"Count\"]>=100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above DataFrame therefore shows who are the pairs of users that interacted the most on a time basis about hashtag **#MakeAmericaGreatAgain**. Please remind that because of the filtering performed in the previous section, DataFrame might include some users that are not part of the graph data.\n",
    "\n",
    "Another perspective may be given instead to highlight which individual users have been involved in the highest number of interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170806</th>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>758215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73224</th>\n",
       "      <td>foxnews</td>\n",
       "      <td>71249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51759</th>\n",
       "      <td>danscavino</td>\n",
       "      <td>66905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86242</th>\n",
       "      <td>hillaryclinton</td>\n",
       "      <td>58393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43898</th>\n",
       "      <td>cnn</td>\n",
       "      <td>46671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201162</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>45448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79782</th>\n",
       "      <td>gop</td>\n",
       "      <td>31977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185091</th>\n",
       "      <td>seanhannity</td>\n",
       "      <td>26622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67933</th>\n",
       "      <td>erictrump</td>\n",
       "      <td>24374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59825</th>\n",
       "      <td>donaldjtrumpjr</td>\n",
       "      <td>20553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132411</th>\n",
       "      <td>marcorubio</td>\n",
       "      <td>17131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137786</th>\n",
       "      <td>megynkelly</td>\n",
       "      <td>16594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140585</th>\n",
       "      <td>mike_pence</td>\n",
       "      <td>16152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142732</th>\n",
       "      <td>mitchellvii</td>\n",
       "      <td>16121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14166</th>\n",
       "      <td>anncoulter</td>\n",
       "      <td>14139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87610</th>\n",
       "      <td>horseshort</td>\n",
       "      <td>13836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43264</th>\n",
       "      <td>clewandowski_</td>\n",
       "      <td>13686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165455</th>\n",
       "      <td>potus</td>\n",
       "      <td>13608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99225</th>\n",
       "      <td>jebbush</td>\n",
       "      <td>13363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146974</th>\n",
       "      <td>msnbc</td>\n",
       "      <td>13315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125273</th>\n",
       "      <td>lindasuhler</td>\n",
       "      <td>13117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228719</th>\n",
       "      <td>youtube</td>\n",
       "      <td>12664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155617</th>\n",
       "      <td>o_irisht</td>\n",
       "      <td>12536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139481</th>\n",
       "      <td>michaelcohen212</td>\n",
       "      <td>12371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>abc</td>\n",
       "      <td>11566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172784</th>\n",
       "      <td>reince</td>\n",
       "      <td>11550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93562</th>\n",
       "      <td>ivankatrump</td>\n",
       "      <td>11214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78771</th>\n",
       "      <td>globalsocialm2</td>\n",
       "      <td>11179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192936</th>\n",
       "      <td>speakerryan</td>\n",
       "      <td>10898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200971</th>\n",
       "      <td>teamtrump</td>\n",
       "      <td>10525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105582</th>\n",
       "      <td>johnkerry</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112481</th>\n",
       "      <td>karltpf</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15768</th>\n",
       "      <td>ariel_isles</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69913</th>\n",
       "      <td>fairdealdavenh</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39721</th>\n",
       "      <td>cher</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70222</th>\n",
       "      <td>fanofgreenmms</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148359</th>\n",
       "      <td>myriamwinner</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90653</th>\n",
       "      <td>im4djtrump</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112283</th>\n",
       "      <td>kareness07</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107546</th>\n",
       "      <td>josephmonaco</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224094</th>\n",
       "      <td>whoopigoldberg</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219163</th>\n",
       "      <td>vanjones68</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42771</th>\n",
       "      <td>clanceman65</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33175</th>\n",
       "      <td>buybyfelicia</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102236</th>\n",
       "      <td>jimbobbarley</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214389</th>\n",
       "      <td>trumpnv</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12165</th>\n",
       "      <td>amsteelpres</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73620</th>\n",
       "      <td>franklin_graham</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179108</th>\n",
       "      <td>rt41racer</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113942</th>\n",
       "      <td>kdredkarl</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141105</th>\n",
       "      <td>mikepencevp</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218899</th>\n",
       "      <td>valenti317</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156872</th>\n",
       "      <td>oldskull65</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226315</th>\n",
       "      <td>wrmilligan</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172534</th>\n",
       "      <td>reese32309</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183286</th>\n",
       "      <td>savagenation</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57638</th>\n",
       "      <td>digonzalez</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176041</th>\n",
       "      <td>rnrcolorado</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54919</th>\n",
       "      <td>debramax</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221379</th>\n",
       "      <td>vp</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   User   Count\n",
       "170806  realdonaldtrump  758215\n",
       "73224           foxnews   71249\n",
       "51759        danscavino   66905\n",
       "86242    hillaryclinton   58393\n",
       "43898               cnn   46671\n",
       "201162          tedcruz   45448\n",
       "79782               gop   31977\n",
       "185091      seanhannity   26622\n",
       "67933         erictrump   24374\n",
       "59825    donaldjtrumpjr   20553\n",
       "132411       marcorubio   17131\n",
       "137786       megynkelly   16594\n",
       "140585       mike_pence   16152\n",
       "142732      mitchellvii   16121\n",
       "14166        anncoulter   14139\n",
       "87610        horseshort   13836\n",
       "43264     clewandowski_   13686\n",
       "165455            potus   13608\n",
       "99225           jebbush   13363\n",
       "146974            msnbc   13315\n",
       "125273      lindasuhler   13117\n",
       "228719          youtube   12664\n",
       "155617         o_irisht   12536\n",
       "139481  michaelcohen212   12371\n",
       "5336                abc   11566\n",
       "172784           reince   11550\n",
       "93562       ivankatrump   11214\n",
       "78771    globalsocialm2   11179\n",
       "192936      speakerryan   10898\n",
       "200971        teamtrump   10525\n",
       "...                 ...     ...\n",
       "105582        johnkerry     518\n",
       "112481          karltpf     518\n",
       "15768       ariel_isles     518\n",
       "69913    fairdealdavenh     517\n",
       "39721              cher     517\n",
       "70222     fanofgreenmms     517\n",
       "148359     myriamwinner     515\n",
       "90653        im4djtrump     514\n",
       "112283       kareness07     512\n",
       "107546     josephmonaco     511\n",
       "224094   whoopigoldberg     511\n",
       "219163       vanjones68     511\n",
       "42771       clanceman65     510\n",
       "33175      buybyfelicia     510\n",
       "102236     jimbobbarley     507\n",
       "214389          trumpnv     507\n",
       "12165       amsteelpres     507\n",
       "73620   franklin_graham     506\n",
       "179108        rt41racer     506\n",
       "113942        kdredkarl     505\n",
       "141105      mikepencevp     504\n",
       "218899       valenti317     504\n",
       "156872       oldskull65     503\n",
       "226315       wrmilligan     503\n",
       "172534       reese32309     503\n",
       "183286     savagenation     502\n",
       "57638        digonzalez     501\n",
       "176041      rnrcolorado     501\n",
       "54919          debramax     500\n",
       "221379               vp     500\n",
       "\n",
       "[811 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = interactions_df.groupby(['Source']).sum().reset_index().rename(columns={'Source':'User'})\n",
    "temp2 = interactions_df.groupby(['Target']).sum().reset_index().rename(columns={'Target':'User'})\n",
    "temp = temp1.append(temp2)\n",
    "temp = temp.groupby(['User']).sum().reset_index()\n",
    "temp.sort_values(by=\"Count\",ascending=False, inplace=True)\n",
    "temp[temp[\"Count\"]>=500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above gives an idea about who have been the most active users around the hashtag topic overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup DataFrames to visualize top interactions with NetworkX / Graphistry\n",
    "interactions_df[interactions_df[\"Count\"]>=50].to_csv(os.path.join(data_dir, \"graph_visualization_top_interactions/\"+hashtag+\"_top_interactions.csv\"),index=False)\n",
    "temp[temp[\"Count\"]>=100].to_csv(os.path.join(data_dir, \"graph_visualization_top_interactions/\"+hashtag+\"_users_frequency.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measuring interactions consistency overtime\n",
    "What I'm now interested in, and what actually represents the end goal of this whole work, is identifying a measure of consistency related to the interactions of people taking part of controversies on Twitter. According to the data I am provided with, I may identify 2 paths:\n",
    "- **MMR graph data**: there's a property of the graph data that hasn't been used so far but still lies in the CSV files I got as result of all the data transformation steps that featured the first notebooks. This property would already give a built-in definition of *interaction consistency*; however, the information we may infer from it is quite limited, and the reason is that for each pair of users we only know whether they have interacted (consistently) within a 3 months period, and given the ranges of our timeline this sums up to a total of 13 periods. It's the only way, however, to compare how interaction consistency has changed overtime;\n",
    "- **Tweets**: when limiting the scope to the tweets related to a specific topic (hashtag), I have no information about which other topics all involved users have been tweeting about neither the related temporal information. Therefore, I may only know when has the *first* Twitter interaction occurred between each pair of mutually interacting users and all the subsequent ones. This is something that has already been highlighted a bit with the previous section, however I may further narrow it down and measure interactions consistency overtime if I count the interactions *per month*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Long-term interaction consistency comparison: Analyzing behavioral changes overtime\n",
    "The idea of this analysis is to compare interaction consistency before and after the first $H$-type interaction, in order to highlight the bonding effect of a given hashtag: does it actually strengthen or weaken ties overtime? Are users more or less likely to communicate with each other after their $H$ interaction? We want to address these questions by carrying out this first timeline analysis by using metadata as edge attributes provided by the MMR graph data.\n",
    "\n",
    "In details: we have a total of 13 periods, ranging from 2013-09 to 2016-12, each of 3 months duration. For each edge in $H$ and for each of the periods, we then have a binary variable with values 1/0, set to 1 if the interaction occurred in the respective 3 months period, 0 otherwise (simplification of True/False values). Let's say, given period with 0-based index $i$ and edge $e$: $IsInPeriod(e,i)$ is a function as defined below:\n",
    "\n",
    "$$\n",
    "IsInPeriod(e,i) =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if $e$ occurred in period $i$,}\\\\\n",
    "        0 & \\text{otherwise.}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Then, the **mutual interaction consistency** of edge $e$, calculated over $P$ consecutive periods starting from period with index $k$ until period with index $j$ (with $j +1 \\ge k$), is given by:\n",
    "\n",
    "$$MIC(e,k,j) = \\frac{\\sum_{i=k}^{j}IsInPeriod(e,i)}{P} = \\frac{\\sum_{i=k}^{j}IsInPeriod(e,i)}{j-k+1}$$\n",
    "\n",
    "Based on the values of $j$ and $k$, $MIC$ becomes meaningful if we consider its **average** over all edges. Therefore, the **average mutual interaction consistency** $\\langle MIC \\rangle$ may be calculated in three similar ways:\n",
    "- $\\langle MIC \\rangle_{T}$: for each edge $e$, $P = 13$, $k=0$, $j=12$ (the whole timeline period);\n",
    "- $\\langle MIC \\rangle_{B}$: for each edge $e$, $P$ is the number of consecutive periods *before* the first $H$ interaction between nodes of edge $e$, i.e. if the first interaction occurred in period with index $q$, then $k=0$, $j=q-1$;\n",
    "- $\\langle MIC \\rangle_{A}$: for each edge $e$, $P$ is the number of consecutive periods *after* the first $H$ interaction between nodes of edge $e$, i.e. $k=q$, $j=12$.\n",
    "\n",
    "In general, given $E$ as the set of edges of $H$ subgraph:\n",
    "\n",
    "$$\\langle MIC \\rangle = \\frac{\\sum_{e \\in E}MIC(e)}{|E|}$$\n",
    "\n",
    "where the values of $k$ and $j$, that depend directly on the considered edge $e$ based on which type of $\\langle MIC \\rangle$ we're calculating, have been encapsulated inside the $MIC$ function.\n",
    "\n",
    "Values of $\\langle MIC \\rangle_{T}$ may then be compared across different $H$ subgraphs, whereas $\\langle MIC \\rangle_{B}$ and $\\langle MIC \\rangle_{A}$ may be then directly compared within the same $H$ subgraph to quantify the bonding impact of hashtag $H$ overtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we need to conveniently store the edges attribute. This metadata is already transformed into an array of integers where values are the encoding of the periods, ranging from 0 to 12, where period $0$ is period between 2013-09 and 2013-11 (edges included),  and period $12$ between 2016-09 and 2016-11.\n",
    "\n",
    "I may reuse the `Interaction` custom type to store them as keys of a dictionary (this time I also add a value for the periods field), and I map each `Interaction` to the period corresponding to the first $H$ interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 43419395 lines...\n",
      "Processed 86838790 lines...\n",
      "Processed 130258185 lines...\n",
      "Processed 173677580 lines...\n",
      "Processed 217096975 lines...\n",
      "Processed 260516370 lines...\n",
      "Processed 303935765 lines...\n",
      "Processed 347355160 lines...\n",
      "Processed 390774555 lines...\n",
      "Processed 434193950 lines...\n",
      "Done!\n",
      "CPU times: user 26min 53s, sys: 3.91 s, total: 26min 57s\n",
      "Wall time: 27min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read MMR encoded file\n",
    "interaction_to_first_occ = {}\n",
    "count = 0\n",
    "tot_lines = !wc -l ../data/mmr_encoded_final.csv\n",
    "tot_lines = int(tot_lines[0].split()[0])\n",
    "checkpoint = tot_lines/10\n",
    "with open(\"../data/mmr_encoded_final.csv\") as fin:\n",
    "    for line in read_large_file(fin):\n",
    "        source, target, periods = process_edge_line(line)\n",
    "        count+=1\n",
    "        if count % checkpoint == 0:\n",
    "            print(\"Processed %d lines...\" %count)\n",
    "        if not hashtag_subgraph.IsEdge(source,target):\n",
    "            continue\n",
    "        interaction_to_first_occ[Interaction(source, target, periods)] = None\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to have a dict that maps each period with the corresponding encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_periods_encoding_dict():\n",
    "    periods_encoding_df = pd.read_csv(\"../data/period_encoding.csv\")\n",
    "    return {r[\"period\"]:r[\"encoding\"] for _,r in periods_encoding_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods_encoding_dict = get_periods_encoding_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, I need a function that given a date returns the period it belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_encoding_by_date(date, periods_encoding_dict):\n",
    "    for period,encoding in sorted(periods_encoding_dict.iteritems(), key=operator.itemgetter(1)):\n",
    "        if date >= datetime.datetime.strptime(period, \"%Y-%m\").date():\n",
    "            result = encoding\n",
    "        else:\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I may scan all the tweets and save the period when each interaction first occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 808 ms, total: 2min 42s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for t in tweets_filtered:\n",
    "    date = datetime.datetime.strptime(t.tweet_dict['created_at'],'%a %b %d %H:%M:%S +0000 %Y').date()\n",
    "    period_encoding = get_period_encoding_by_date(date, periods_encoding_dict)\n",
    "    potential_edges = list(combinations(t.users, 2))\n",
    "    for e in potential_edges:\n",
    "        if e[0] in usernames_to_id_dict and e[1] in usernames_to_id_dict:\n",
    "            interaction = Interaction(usernames_to_id_dict[e[0]],usernames_to_id_dict[e[1]])\n",
    "            if interaction in interaction_to_first_occ and (interaction_to_first_occ[interaction] is None or period_encoding < interaction_to_first_occ[interaction]):\n",
    "                interaction_to_first_occ[interaction] = period_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions that have been captured by MMR graph data: 57641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total interactions that have been captured by MMR graph data: %d\" %len([k for k in interaction_to_first_occ if not interaction_to_first_occ[k] is None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_to_first_occ = {k:interaction_to_first_occ[k] for k in interaction_to_first_occ if not interaction_to_first_occ[k] is None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Average Mutual Interaction Consistency (Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_mic_total(interactions_dict):\n",
    "    s = pd.Series([len(i.periods)*1.0/13 for i in interactions_dict], dtype=np.float32)\n",
    "    avg = s.describe()[\"mean\"]\n",
    "    return avg, pd.DataFrame(s, columns=[\"MIC Total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mutual Interaction Consistency (Total) for hashtag #MAGA: 0.194798\n"
     ]
    }
   ],
   "source": [
    "avg_mic_total, mic_total_df = calculate_avg_mic_total(interaction_to_first_occ)\n",
    "mic_total_df.to_csv(\"../data/mic_data/mic_total_\"+hashtag+\".csv\", index=False)\n",
    "print(\"Average Mutual Interaction Consistency (Total) for hashtag %s: %.6f\" %(hashtag_full, avg_mic_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Average  Mutual Interaction Consistency (Before vs After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_mic_before_after(interactions_dict):\n",
    "    mic_before_list = []\n",
    "    mic_after_list = []\n",
    "    for i in interactions_dict:\n",
    "        first_interaction = interactions_dict[i]\n",
    "        sorted_periods = sorted(i.periods)\n",
    "        periods_before = len(range(0,first_interaction))\n",
    "        mic_before = len([p for p in sorted_periods if p < first_interaction])*1.0/periods_before if periods_before!=0 else 0\n",
    "        mic_before_list.append(mic_before)\n",
    "        mic_after_list.append(len([p for p in sorted_periods if p >= first_interaction])*1.0/len(range(first_interaction,13)))\n",
    "    s_before = pd.Series(mic_before_list, name=\"MIC Before\", dtype=np.float32)\n",
    "    s_after = pd.Series(mic_after_list, name=\"MIC After\", dtype=np.float32)\n",
    "    avg_mic_before = s_before.describe()[\"mean\"]\n",
    "    avg_mic_after = s_after.describe()[\"mean\"]\n",
    "    return avg_mic_before, avg_mic_after, pd.concat([s_before, s_after], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mutual Interaction Consistency (Before) for hashtag #MAGA: 0.122399\n",
      "Average Mutual Interaction Consistency (After) for hashtag #MAGA: 0.434009\n"
     ]
    }
   ],
   "source": [
    "avg_mic_before, avg_mic_after, mic_before_after_df = calculate_avg_mic_before_after(interaction_to_first_occ)\n",
    "mic_before_after_df.to_csv(\"../data/mic_data/mic_before_after_\"+hashtag+\".csv\", index=False)\n",
    "print(\"Average Mutual Interaction Consistency (Before) for hashtag %s: %.6f\" %(hashtag_full, avg_mic_before))\n",
    "print(\"Average Mutual Interaction Consistency (After) for hashtag %s: %.6f\" %(hashtag_full, avg_mic_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Property | Value\n",
    "|---|---|\n",
    "| $\\langle MIC \\rangle_{T}$ | 0.194798\n",
    "| $\\langle MIC \\rangle_{B}$ | 0.122399\n",
    "| $\\langle MIC \\rangle_{A}$ | 0.434009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to investigate also for how many interactions there has been an improvement in terms of bonding effect, meaning that the two users have been interacting more frequently than they had before because of the $H$ interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_before_after_df[\"Bonding\"] = mic_before_after_df[\"MIC After\"] > mic_before_after_df[\"MIC Before\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     57641\n",
       "unique        2\n",
       "top        True\n",
       "freq      45569\n",
       "Name: Bonding, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_before_after_df[\"Bonding\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Detailed Interaction consistency: how long do ties survive?\n",
    "This last step of the timeline analysis has the goal to provide a detailed overview of the frequency of the interactions of users for a specific hashtag per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
