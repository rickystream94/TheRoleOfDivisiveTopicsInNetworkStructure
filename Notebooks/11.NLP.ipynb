{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing on Tweets: Sentiment Analysis and text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Set Seaborn defaults\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.precision\", 6)\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "mpl.rcParams['savefig.dpi'] = 150\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "# Global variables\n",
    "data_dir = \"../data\"\n",
    "pictures_path = os.path.join(\"../Pictures\", \"11.NLP\")\n",
    "tweets_path = \"../lib/GetOldTweets-python/out/completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes\n",
    "class Tweet:\n",
    "    def __init__(self, tweet_id, tweet_dict):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.tweet_dict = tweet_dict\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Tweet):\n",
    "            return self.tweet_id == other.tweet_id\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        x = self.__eq__(other)\n",
    "        if x is not NotImplemented:\n",
    "            return not x\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def get_relative_percentage(n,m):\n",
    "    return n*100.0/m\n",
    "\n",
    "def read_large_file(file_object):\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data.rstrip('\\n')\n",
    "        \n",
    "# Extract tweets given a specific hashtag (include also retweeted/quoted tweets)\n",
    "def get_tweets(hashtag):\n",
    "    tweets_filename = os.path.join(tweets_path,\"tweets_#\" + hashtag + \"_2013-09-01_2016-12-31.json\")\n",
    "    tweets = set()\n",
    "    with open(tweets_filename) as fin:\n",
    "        for line in read_large_file(fin):\n",
    "            tweet_dict = json.loads(line)\n",
    "            tweet_id = np.int64(tweet_dict[\"id_str\"])\n",
    "            t = Tweet(tweet_id, tweet_dict)\n",
    "            for special in [\"retweeted_status\",\"quoted_status\"]:\n",
    "                if special in tweet_dict:\n",
    "                    tweets.add(Tweet(np.int64(tweet_dict[special][\"id_str\"]), tweet_dict[special]))\n",
    "                    t.tweet_dict.pop(special)\n",
    "            tweets.add(t)\n",
    "    print(\"Imported %d tweets from %s\" %(len(tweets),tweets_filename))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sentiment Analysis and Tweets text classification is a common task among NLP-ers although highly subject to misclassification risks due to the ambiguous nature of the text content and the actual sentiment they want to transmit. It's not uncommon, indeed, to see sarcastic Tweets whose actual sentiment is not trivial to be inferred by a classifier that has been trained on pure positive/negative/neutral emotions. \n",
    "\n",
    "With my analysis, I want to achieve the following goals:\n",
    "1. **Compute Sentiment Analysis Score for different sets of hashtags**: Pick sets of Tweets of hashtags that are strongly left/right leaning (e.g. #ImWithHer and #BLM are left leaning or democrats/liberals, whereas #MAGA and #ALM are right-leaning or republicans/conservatives) and compute average sentiment score (first by using the built-in classifier provided by TextBlob pre-trained on movies reviews, secondly using a custom classifier trained ad-hoc on actual Tweets) for each of them and show Sentiment Score distribution with boxplots where observations are individual tweets; show basic stats (e.g. percentage of tweets with positive/negative sentiment, show the top positive/negative tweets);\n",
    "2. **Political Text classification**: Build corpus of tweet texts for each category (Right, Left for simplicity) and build classifier to predict political orientation (polarity) by content of tweet, using bag-of-words/TF-IDF approaches. This would not be a perfect political tendency classifier, mainly because the content of each tweet related to a certain hashtag might not entirely reflect an opinion on *that* hashtag.\n",
    "\n",
    "Predicting political tendency of user based both on the content of the tweets and its role and properties in the network structure is a task that requires more complicated steps and an accurate methodology for which I have no time left to be spent.\n",
    "\n",
    "## 1. Sentiment Analysis\n",
    "I've chosen to work mainly with the `TextBlob` Python package, which basically exposes a wrapper for the well-known `NLTK` (Natural Language ToolKit) package, largely adopted for Natural Language Processing tasks.\n",
    "\n",
    "### 1.1 Pre-Processing\n",
    "Tweets often include noisy data that ideally should be cleaned up prior to any analysis. Luckily, for the sake of the sentiment analysis, `TextBlob` comes handy as it offers most of the required pre-processing tasks out of the box and are already automatically performed when calculating the sentiment score. The pre-processing tasks I would consider include (in order):\n",
    "- Converting the text to lowercase\n",
    "- Translation of non-english tweets *(ignored because of performance degradation)*\n",
    "- URL stripping *(this has already been applied by the script used to download the Tweets)*\n",
    "- Removing non-ASCII characters, symbols, numbers\n",
    "- Removing User mentions\n",
    "- Spell-checking and correction *(ignored because of performance degradation)*\n",
    "- Removing stopwords\n",
    "- Filtering out short words (e.g. $len(w) \\leq 3$)\n",
    "- Emoticon detection (already nicely handled by default by `TextBlob`)\n",
    "- Stemming and lemmatisation, to group together the inflected forms of a word so they can be analyzed as a single item\n",
    "- Perform POS (Part-Of-Speech) tagging to select only relevant features/tokens (like nouns, verbs, adjectives)\n",
    "\n",
    "`TextBlob`, luckily, already performs most of the text parsing and pre-processing listed above before computing the sentiment score. However, we still want to perform all of these in order to perform further text analysis. In order to perform sentiment analysis without loosing information provided by punctuation, I define two functions to process the tweets text as defined above. One would be the *baseline cleaner function* (enough prior to carrying out the SA task) and the other one the *advanced cleaning function* (to be used prior to the text classification task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1745238 tweets from ../lib/GetOldTweets-python/out/completed/tweets_#makeamericagreatagain_2013-09-01_2016-12-31.json\n",
      "Imported 3189115 tweets from ../lib/GetOldTweets-python/out/completed/tweets_#imwithher_2013-09-01_2016-12-31.json\n",
      "CPU times: user 8min 3s, sys: 12.5 s, total: 8min 16s\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read tweets\n",
    "hashtags = set([\"imwithher\", \"makeamericagreatagain\"])\n",
    "hashtag_to_tweets = {h:get_tweets(h) for h in hashtags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first show some basic stats on the tweets, for example which are the most common hashtags that are used along with the main ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 56 ms, total: 1.56 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extra_hashtags = {}\n",
    "for h in hashtag_to_tweets:\n",
    "    extra_hashtags[h] = {}\n",
    "    for t in hashtag_to_tweets[h]:\n",
    "        for hashtag in t.tweet_dict[\"entities\"][\"hashtags\"]:\n",
    "            if hashtag.lower() == h:\n",
    "                continue\n",
    "            if hashtag.lower() in extra_hashtags[h]:\n",
    "                extra_hashtags[h][hashtag.lower()] += 1\n",
    "            else:\n",
    "                extra_hashtags[h][hashtag.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-----------+\n",
      "| Supporting Hashtags (jesuischarlie) | Frequency |\n",
      "+-------------------------------------+-----------+\n",
      "|             charliehebdo            |   44384   |\n",
      "|               montr√©al              |   25791   |\n",
      "|                polqc                |   17424   |\n",
      "|             islamisation            |   16507   |\n",
      "|                paris                |   15503   |\n",
      "|               letter4u              |    8421   |\n",
      "|              letter4all             |    8399   |\n",
      "|              musulmans              |    7564   |\n",
      "|               hadiths               |    7471   |\n",
      "|             jesuisahmed             |    6188   |\n",
      "|                france               |    5449   |\n",
      "|          noussommescharlie          |    4599   |\n",
      "|              jesuisjuif             |    4119   |\n",
      "|             prisedotage             |    3100   |\n",
      "|             jesuisparis             |    2906   |\n",
      "|                islam                |    2899   |\n",
      "|              notafraid              |    2730   |\n",
      "|               charlie               |    2577   |\n",
      "|             parisattacks            |    2556   |\n",
      "|             notinmyname             |    2248   |\n",
      "+-------------------------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "for h in extra_hashtags:\n",
    "    top_20 = sorted(extra_hashtags[h].items(), key=operator.itemgetter(1), reverse=True)[:20]\n",
    "    t = PrettyTable(['Supporting Hashtags (%s)' %h, 'Frequency'])\n",
    "    for el in top_20:\n",
    "        t.add_row(el)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define all the functions needed for the pre-processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_tweet_baseline(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet[\"text\"] = tweet[\"text\"].lower()\n",
    "    \n",
    "    # Convert non-English texts to English: translation takes up a lot of execution time\n",
    "    '''if tweet[\"lang\"] != \"en\":\n",
    "        try:\n",
    "            tweet[\"text\"] = TextBlob(tweet[\"text\"]).translate(from_lang=tweet[\"lang\"], to=\"en\").string\n",
    "        except:\n",
    "            pass'''\n",
    "        \n",
    "    # Remove handles (user mentions)\n",
    "    tweet[\"text\"] = remove_pattern(tweet[\"text\"], r\"@[\\w]*\")\n",
    "    \n",
    "    # Remove hashtag symbol as prefix for hashtags\n",
    "    tweet[\"text\"] = tweet[\"text\"].replace('#','')\n",
    "    \n",
    "    # Fix classic slang/internet abbreviations\n",
    "    abbreviations = [(r'\\bthats\\b','that is'),(r'\\bive\\b','i have'),(r'\\bim\\b','i am'),(r'\\bya\\b','yeah'),\n",
    "                     (r'\\bcant\\b','can not'),(r'\\bwont\\b','will not'),(r'\\bid\\b','i would'),(r'wtf','what the fuck'),(r'\\bwth\\b','what the hell'),\n",
    "                     (r'\\br\\b','are'),(r'\\bu\\b','you'),(r'\\bk\\b','OK'),(r'\\bsux\\b','sucks'),(r'\\bno+\\b','no'),(r'\\bcoo+\\b','cool'),\n",
    "                     (r'\\blol\\b','lot of laughs')]\n",
    "    for abb in abbreviations:\n",
    "        tweet['text'] = re.sub(abb[0], abb[1], tweet[\"text\"])\n",
    "        \n",
    "    # Strip non ASCII characters\n",
    "    tweet[\"text\"] = \"\".join((c for c in tweet[\"text\"] if 0 < ord(c) < 127))\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet[\"text\"] = re.sub(r'\\d+', '', tweet[\"text\"])\n",
    "    \n",
    "    # Remove HTML symbols (like '&', tabs etc.)\n",
    "    tweet[\"text\"] = re.sub(r'&\\w+;', '', tweet[\"text\"])\n",
    "    \n",
    "    # Remove short words (len <= 3)\n",
    "    tweet[\"text\"] = \" \".join([w for w in tweet[\"text\"].split() if len(w.translate({ord(c): None for c in string.punctuation}))>3])\n",
    "    \n",
    "    # Remove stop words\n",
    "    tweet[\"text\"] = \" \".join([w for w in tweet[\"text\"].split() if w.translate({ord(c): None for c in string.punctuation}) not in set(stopwords.words('english'))])\n",
    "    \n",
    "    tweet[\"text\"] = tweet[\"text\"].strip()\n",
    "    return tweet\n",
    "    \n",
    "def clean_tweet_advanced(tweet):\n",
    "    # Singularize words (remove plurals)\n",
    "    tweet[\"text\"] = \" \".join([w.singularize() for w in TextBlob(tweet[\"text\"]).words])\n",
    "    \n",
    "    # Filter by relevant POS tags and lemmatize\n",
    "    pos_tags = [(w,get_wordnet_pos(pos_tag)) for w,pos_tag in TextBlob(tweet[\"text\"]).pos_tags if get_wordnet_pos(pos_tag) != '']\n",
    "    tweet[\"text\"] = \" \".join([w.lemmatize(pos_tag) for w,pos_tag in pos_tags])\n",
    "    \n",
    "    tweet[\"text\"] = tweet[\"text\"].strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up execution times, I'm forced to skip tweet text translation for non-english tweets, as it takes up a lot of execution time due to the network communication required by the translator. Furthermore, only tweets that actually express a logically complete and reasonable thought may be useful for our analysis, hence short tweets may be discarded. Let's then filter the tweets and include only the ones with english text and that have at least 10 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makeamericagreatagain: Number of tweets before filtering: 1745238\n",
      "makeamericagreatagain: Number of tweets after filtering: 1017104 (58.28% of 1745238)\n"
     ]
    }
   ],
   "source": [
    "for h in hashtag_to_tweets:\n",
    "    len_before = len(hashtag_to_tweets[h])\n",
    "    print(\"%s: Number of tweets before filtering: %d\" %(h,len_before))\n",
    "    hashtag_to_tweets[h] = {t for t in hashtag_to_tweets[h] if t.tweet_dict[\"lang\"] == \"en\" and len(TextBlob(t.tweet_dict[\"text\"]).words) >= 10}\n",
    "    len_after = len(hashtag_to_tweets[h])\n",
    "    print(\"%s: Number of tweets after filtering: %d (%.2f%% of %d)\" %(h,len_after,get_relative_percentage(len_after,len_before),len_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally clean the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 1000 tweets\n",
      "Cleaned 2000 tweets\n",
      "Cleaned 3000 tweets\n",
      "Cleaned 4000 tweets\n",
      "Cleaned 5000 tweets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-65365e95203a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhashtag_to_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweet_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mhashtag_to_cleaned_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-b1dd01ee261f>\u001b[0m in \u001b[0;36mclean_tweet_baseline\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/s161155/anaconda2/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/wordlist.pyc\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/s161155/anaconda2/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/wordlist.pyc\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/s161155/anaconda2/envs/py27/lib/python2.7/site-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/s161155/anaconda2/envs/py27/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashtag_to_cleaned_tweets = {}\n",
    "for h in hashtag_to_tweets:\n",
    "    hashtag_to_cleaned_tweets[h] = set()\n",
    "    count = 0\n",
    "    for t in hashtag_to_tweets[h]:\n",
    "        t.tweet_dict = clean_tweet_baseline(t.tweet_dict)\n",
    "        hashtag_to_cleaned_tweets[h].add(t)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Cleaned %d tweets\" %count)\n",
    "\n",
    "#hashtag_to_cleaned_tweets = {h:{clean_tweet_baseline(t.tweet_dict) for t in hashtag_to_tweets[h]} for h in hashtag_to_tweets} # Equivalent 1-liner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
