{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilities**:\n",
    "- Windows keystroke for \\` character is ALT+96\n",
    "- Windows keystroke for ~ character is ALT+126\n",
    "\n",
    "**Notebook Initialization**: Hereby all the needed `import` statements, global variables or functions with global scope throughout the notebook.\n",
    "**PySpark** has been used for data processing to avoid overloading memory usage and take advantage of the benefits from the **RDD** (Resilient Distributed Dataset) format to deal with huge files without the explicit need of loading all their contents into memory.\n",
    "\n",
    "Run the cell below once to initialize all the needed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import os\n",
    "import sys\n",
    "import networkx as nx\n",
    "import operator\n",
    "import json\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "%load_ext Cython\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize PySpark\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Global Settings\n",
    "dataset_dir = \"../mmr_graph/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Storing Twitter usernames without any transformation in memory would mean huge memory consumption to store a relatively expensive data type such as strings. I could optimize this by performing some *data encoding* on the usernames and convert strings to integers by keeping a 1:1 mapping between the string representation of the username and its integer representation. This would result in huge memory-usage optimization when loading the graph into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Usernames Encoding\n",
    "Since I am dealing with huge files (19GB overall for the MMR dataset) I have to pay attention with what is actually loaded into memory, although the current server configuration offers 128GB available RAM. As a result of the encoding process, I want to create a .CSV file that will store the 1:1 mapping between the (string) Twitter username and its integer encoding. As a rule of thumb, it's just easy to start counting from 0 and assign a unique ID to each username. Output is saved to a `usernames.csv` file. I also wanted to create an encoded copy of the MMR dataset in the form of a single .CSV file, which would then be also easier to parse. It's also just convenient to have a single dataset file as a result of the encoding process, named `mmr_encoded.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "usernames_filename = \"usernames.csv\"\n",
    "usernames_header_filename = \"usernames_header.csv\"\n",
    "mmr_encoded_filename = \"mmr_encoded.csv\"\n",
    "csv_header = [\"username\", \"encoding\"]\n",
    "\n",
    "# Support functions declarations\n",
    "def add_or_get_new_user_encoding(username):\n",
    "    global current_user_id\n",
    "    global usernames_encoding\n",
    "    if not username in usernames_encoding.keys():\n",
    "        user_enc = current_user_id\n",
    "        usernames_encoding[username] = user_enc\n",
    "        current_user_id += 1\n",
    "        if current_user_id%10000000 == 0:\n",
    "            print(\"Currently processed {0} unique usernames\".format(current_user_id))\n",
    "        return user_enc\n",
    "    return usernames_encoding[username]\n",
    "\n",
    "def get_encoding(username):\n",
    "    usernames_rdd = sc.textFile(usernames_filename)\n",
    "    return usernames_rdd.map(lambda x: x.split(',')).filter(lambda x: x[0] == username).map(lambda x: int(x[1])).first()\n",
    "\n",
    "def get_username(encoding):\n",
    "    usernames_rdd = sc.textFile(usernames_filename)\n",
    "    return usernames_rdd.map(lambda x: x.split(',')).filter(lambda x: x[1] == encoding).map(lambda x: x[0]).first()\n",
    "\n",
    "def process_line(line):\n",
    "    a,b = line.strip(\"()\\n\").split(', ')\n",
    "    a = a.strip(\"u'\")\n",
    "    b = b.strip(\"u'\")\n",
    "    a_enc = add_or_get_new_user_encoding(a)\n",
    "    b_enc = add_or_get_new_user_encoding(b)\n",
    "    return a_enc, b_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicating dataset in encoded format...\n",
      "Processing files in folder 2016-03...\n",
      "Currently processed 10000000 unique usernames\n",
      "Processing files in folder 2016-09...\n",
      "Currently processed 20000000 unique usernames\n",
      "Processing files in folder 2013-12...\n",
      "Currently processed 30000000 unique usernames\n",
      "Currently processed 40000000 unique usernames\n",
      "Processing files in folder 2014-06...\n",
      "Currently processed 50000000 unique usernames\n",
      "Processing files in folder 2013-09...\n",
      "Currently processed 60000000 unique usernames\n",
      "Processing files in folder 2014-09...\n",
      "Processing files in folder 2014-12...\n",
      "Processing files in folder 2015-12...\n",
      "Currently processed 70000000 unique usernames\n",
      "Processing files in folder 2016-06...\n",
      "Processing files in folder 2014-03...\n",
      "Currently processed 80000000 unique usernames\n",
      "Processing files in folder 2015-03...\n",
      "Processing files in folder 2015-06...\n",
      "Processing files in folder 2015-09...\n",
      "Dumping encoded usernames to usernames.csv...\n",
      "Processed all usernames. Total unique usernames: 89486650\n",
      "CPU times: user 40min 42s, sys: 28.7 s, total: 41min 10s\n",
      "Wall time: 41min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def start_encoding():\n",
    "    print(\"Duplicating dataset in encoded format...\")\n",
    "    with open(mmr_encoded_filename, \"w\") as mmr_enc_f:\n",
    "        for folder in os.listdir(dataset_dir):\n",
    "            folder_path = os.path.join(dataset_dir, folder)\n",
    "            print(\"Processing files in folder {0}...\".format(folder))\n",
    "            for part in os.listdir(folder_path):\n",
    "                part_path = os.path.join(folder_path, part)\n",
    "                if os.path.isfile(part_path) and part.startswith(\"part\"):\n",
    "                    with open(part_path, encoding=\"utf-8\") as part:\n",
    "                        for line in part:\n",
    "                            a_enc, b_enc = process_line(line)\n",
    "                            mmr_enc_f.write(\",\".join([str(a_enc),str(b_enc)]) + \"\\n\")\n",
    "\n",
    "    print(\"Writing CSV header to separate file {0}...\".format(usernames_header_filename))\n",
    "    with open(usernames_header_filename, \"w\") as usernames_header_f:\n",
    "        usernames_header_f.write(\",\".join(csv_header)+\"\\n\")\n",
    "    print(\"Dumping encoded usernames to {0}...\".format(usernames_filename))\n",
    "    with open(usernames_filename, \"w\") as usernames_out_f:\n",
    "        for k in usernames_encoding.keys():\n",
    "            usernames_out_f.write(\",\".join([k, str(usernames_encoding[k])])+\"\\n\")\n",
    "    print(\"Processed all usernames. Total unique usernames: {0}\".format(current_user_id-1))\n",
    "\n",
    "# Start processing\n",
    "current_user_id = 0 #Start with ID = 0\n",
    "usernames_encoding = {}\n",
    "\n",
    "# If any of the expected output files are missing, run the cell block.\n",
    "if not os.path.exists(usernames_filename) or not os.path.exists(mmr_encoded_filename):\n",
    "    start_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup uneeded variables\n",
    "del usernames_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the encoding process outlined above, these are the two output files and their respective size that will be used from now on:\n",
    "\n",
    "| `usernames.csv` | `mmr_encoded.csv`   |\n",
    "|---|---|\n",
    "|   1.8GB         |                9.7GB|\n",
    "\n",
    "## Building MMR network with Python NetworkX\n",
    "In the first place, I took into consideration the python module NetworkX which turned out to be convenient and easy to use to do network analysis. However, it didn't take much to figure out that its flexibility eventually represent a downside in terms of memory consumption. By using the built-in function `read_edgelist()` to read a graph into memory from the previously generated `mmr_encoded.csv` file, I soon realized that this library is not the right one to solve this problem. I had to break the execution, since memory utilization reached a peak of **125GB** while the graph was not even fully loaded.\n",
    "\n",
    "`G = nx.read_edgelist(mmr_encoded_filename, delimiter=',', nodetype=int, data=False)`\n",
    "\n",
    "By exploring the different alternatives I could use, I ended up finding out that NetworkX is extremely memory inefficient due to the fact that it's a pure Python module. Instead, I started considering [Graph-Tool](https://graph-tool.skewed.de/) a valid alternative and, fascinated by its positive performance benchmarks compared to NetworkX, tried to install it, although users were already warned about it as being extremely cumbersome to install. Unluckily, this turned out to be true: Graph-Tool is written in pure C++, and provides a wrapper for Python. Due to its nature, therefore, it requires the user to manually install all the required dependencies, and follow the `configure / make / make install` pattern, typical of the C++ libraries. I have spent two full days trying to successfully compile and install it, but I eventually reached a dead-end. Importing the library at run-time fails due to (probably) some compilation issues with some of the dependencies (Boost libraries). After several attempts with upgrading/downgrading multiple dependencies and tweaking some compilation parameters, I eventually gave up, also due to the lack of any more useful information retrievable online to help me fix my issues.\n",
    "\n",
    "As a next step, I started considering the other valid alternatives to start out the analysis without the burden of hardcore / low level installation requirements.\n",
    "- [JuliaGraphs](https://juliagraphs.github.io/), which requires the usage of a new kernel for JupyterLab;\n",
    "- [Neo4j](https://neo4j.com/), which relies on the Graph Database model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for Username-Encoding\n",
    "As a reference and last step of this Jupyter Notebook, I include how one can, given a username/encoding, efficiently query for its corresponding encoding/username by using both **PySpark RDDs and Dataframes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 7.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4578983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using RDD\n",
    "get_encoding(\"barackobama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 49s ± 398 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "CPU times: user 292 ms, sys: 56 ms, total: 348 ms\n",
      "Wall time: 34min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using a DataFrame\n",
    "usernames_rdd = sc.textFile(usernames_filename)\n",
    "df_fields = [StructField(csv_header[0], StringType(), False), StructField(csv_header[1], IntegerType(), False)]\n",
    "df_schema = StructType(df_fields)\n",
    "header_rdd = usernames_rdd.filter(lambda l: \",\".join(csv_header) in l)\n",
    "usernames_noHeader_rdd = usernames_rdd.subtract(header_rdd)\n",
    "usernames_df = usernames_noHeader_rdd.map(lambda l: l.split(',')).map(lambda p: (p[0], int(p[1]))).toDF(df_schema)\n",
    "%timeit usernames_df.where(usernames_df.username == \"barackobama\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stats above probably show that PySpark dataframe is much slower than the RDD version, therefore for the easy purpose as retrieving the username / encoding of a given input we can just safely rely on the first option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
