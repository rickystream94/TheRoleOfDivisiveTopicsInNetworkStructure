{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline Analysis\n",
    "Scope of the following analysis is to investigate whether the previously analyzed hashtags actually play a significant role in the underlying network structure: does a specific topic (hashtag) strengthen ties or increases diversity within Twitter users? The idea is therefore to build a metric and extract this information by combining mutual interactions Tweets and the network structure that represented the sole core of the analysis so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snap\n",
    "from snap import TUNGraph\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import seaborn as sns\n",
    "from __future__ import print_function\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "# Set Seaborn defaults\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "mpl.rcParams['savefig.dpi'] = 100\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "# Global variables\n",
    "data_dir = \"../data\"\n",
    "pictures_path = os.path.join(\"../Pictures\", \"8.TimelineAnalysis\")\n",
    "tweets_path = \"../lib/GetOldTweets-python/out/completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Creation of Model Classes\n",
    "First of all, I might need to be a bit more concerned about performance since some Tweets files are pretty big (a few GBs), therefore I'd like to optimize some operations as much as possible.\n",
    "- Since Tweets come naturally with a unique ID, I may create a `Tweet` custom type;\n",
    "- Class `Interaction` representing an interaction between 2 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, tweet_id, users, tweet_dict):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.tweet_dict = tweet_dict\n",
    "        self.users = users\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Tweet):\n",
    "            return self.tweet_id == other.tweet_id\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        x = self.__eq__(other)\n",
    "        if x is not NotImplemented:\n",
    "            return not x\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Interaction):\n",
    "            return (self.source == other.source and self.target == other.target) or (self.source == other.target and self.target == other.source)\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        x = self.__eq__(other)\n",
    "        if x is not NotImplemented:\n",
    "            return not x\n",
    "        return NotImplemented\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(hash(self.source)+hash(self.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby a collection of some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_percentage(n,m):\n",
    "    return n*100.0/m\n",
    "\n",
    "def load_graph_from_backup(filename):\n",
    "    FIn = snap.TFIn(\"../data/\"+filename+\".bin\")\n",
    "    graph = snap.TUNGraph.Load(FIn)\n",
    "    return graph\n",
    "\n",
    "def read_large_file(file_object):\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data.rstrip('\\n')\n",
    "        \n",
    "def process_edge_line(line):\n",
    "    source, target, prop = line.split(',')\n",
    "    return int(source), int(target), prop\n",
    "        \n",
    "def get_usernames_from_basic_tweet_info(hashtag, tweet):\n",
    "    usernames = set()\n",
    "    # (1): Has tweeted using hashtag\n",
    "    if hashtag in [h.lower() for h in tweet[\"entities\"][\"hashtags\"]]:\n",
    "        usernames.add(tweet[\"user\"][\"screen_name\"].lower())\n",
    "\n",
    "    # (2): Has been mentioned / replied to\n",
    "    if not tweet[\"in_reply_to_screen_name\"] is None:\n",
    "        usernames.add(tweet[\"in_reply_to_screen_name\"].lower())\n",
    "    for mentions in tweet[\"entities\"][\"user_mentions\"]:\n",
    "        usernames.add(mentions[\"screen_name\"].lower())\n",
    "    return usernames\n",
    "\n",
    "def get_tweet_usernames(hashtag, tweet):\n",
    "    usernames = set()\n",
    "    usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet))\n",
    "    if \"retweeted_status\" in tweet:\n",
    "        usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet[\"retweeted_status\"]))\n",
    "    if \"quoted_status\" in tweet:\n",
    "        usernames.update(get_usernames_from_basic_tweet_info(hashtag, tweet[\"quoted_status\"]))\n",
    "    return usernames\n",
    "\n",
    "def extract_hashtag_usernames(hashtag, tweets):\n",
    "    hashtag_usernames = set()\n",
    "    for tweet in tweets:\n",
    "        hashtag_usernames.update(tweet.users)\n",
    "    print(\"Total unique usernames involved in '#%s' hashtag conversations from %d tweets: %d\" %(hashtag, len(tweets), len(hashtag_usernames)))\n",
    "    return hashtag_usernames\n",
    "\n",
    "# Extract tweets given a specific hashtag\n",
    "def get_tweets(hashtag):\n",
    "    tweets_filename = os.path.join(tweets_path,\"tweets_#\" + hashtag + \"_2013-09-01_2016-12-31.json\")\n",
    "    tweets = set()\n",
    "    with open(tweets_filename) as fin:\n",
    "        for line in read_large_file(fin):\n",
    "            tweet_dict = json.loads(line)\n",
    "            tweet_id = np.int64(tweet_dict[\"id_str\"])\n",
    "            tweet_users = get_tweet_usernames(hashtag, tweet_dict)\n",
    "            tweets.add(Tweet(tweet_id, tweet_users, tweet_dict))\n",
    "    print(\"Imported %d tweets from %s\" %(len(tweets),tweets_filename))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Tweets: filter usernames and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"jesuischarlie\"\n",
    "hashtag_full = \"#JeSuisCharlie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hashtag subgraph from backup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_subgraph = load_graph_from_backup(\"mmr_subgraph_\"+hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 413857 tweets from ../lib/GetOldTweets-python/out/completed/tweets_#jesuischarlie_2013-09-01_2016-12-31.json\n",
      "CPU times: user 20.5 s, sys: 1.05 s, total: 21.5 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets = get_tweets(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique usernames involved in '#jesuischarlie' hashtag conversations from 413857 tweets: 224646\n"
     ]
    }
   ],
   "source": [
    "hashtag_usernames = extract_hashtag_usernames(hashtag, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 400 ms, total: 2min 45s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "usernames_to_id_dict = {}\n",
    "with open(\"../data/usernames.csv\") as usernames_f:\n",
    "    for line in read_large_file(usernames_f):\n",
    "        username = line.split(',')[0]\n",
    "        encoding = int(line.split(',')[1])\n",
    "        # Add to dict only if username is part of the hashtag subgraph\n",
    "        if hashtag_subgraph.IsNode(encoding):\n",
    "            usernames_to_id_dict[username] = encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is then to filter out those Tweets whose involved users are not part of the corresponding $H$ subgraph (i.e. if none of its involved users represents a node in $H$). The keys of the `usernames_to_id_dict` dictionary correspond to all the usernames of the $H$ subgraph, so it should be sufficient to check the following criteria:\n",
    "- A tweet is kept if any of its involved users is part of the $H$ subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered tweets (with at least 1 involved user within MMR graph data): 218940 (52.90% of 413857 total tweets)\n"
     ]
    }
   ],
   "source": [
    "tweets_filtered = filter(lambda t: any(map(lambda u: u in usernames_to_id_dict, t.users)), tweets)\n",
    "print(\"Number of filtered tweets (with at least 1 involved user within MMR graph data): %d (%.2f%% of %d total tweets)\" %(len(tweets_filtered), get_relative_percentage(len(tweets_filtered), len(tweets)), len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above: among 413.857 total tweets, **218.940** involve at least one user that is part of the MMR graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users involved in 218940 filtered tweets: 156245\n"
     ]
    }
   ],
   "source": [
    "def count_tot_users(tweets):\n",
    "    tot_users = set()\n",
    "    for t in tweets:\n",
    "        tot_users.update(t.users)\n",
    "    return len(tot_users)\n",
    "\n",
    "print(\"Total number of unique users involved in %d filtered tweets: %d\" %(len(tweets_filtered), count_tot_users(tweets_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Of course the number of users involved in the filtered tweets may be higher than the actual number of nodes in $H$, since each Tweet might include users that have not been captured by the MMR data. The reason why I decided to apply this filtering step, is that the users collected by the MMR graph data hide some extra interaction properties, e.g. 2 users are related if their total number of interactions within a 3 months period is sufficiently high, so that it represents a constant interaction over time and not a random one. This way we would already remove a lot of noisy/irrelevant data and speed up the algorithms in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. User Interactions Statistics\n",
    "I could first show some statistics about the interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Top Interactions\n",
    "I may show which are the interactions that occurred the most throughout the years we're considering. It's convenient to create a dictionary with $K \\rightarrow V$ pairs where $K$ = interaction, $V$ = count (count how many times two people have interacted with each other within the whole timeline period we're considering).\n",
    "\n",
    "Collecting the interactions count dictionary may be done linearly with a single pass, and the steps are hereby summarized:\n",
    "1. For each tweet, create a list of all possible pairs of its involved users and for each of them create an `Interaction` instance;\n",
    "2. Add a new entry to the dictionary with value 1, if the interaction is not existing;\n",
    "3. If the interaction is already existing, increment its value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions_count_dict(tweets):\n",
    "    interactions_count = {}\n",
    "    for t in tweets:\n",
    "        pairs = list(combinations(t.users, 2))\n",
    "        for p in pairs:\n",
    "            i = Interaction(p[0], p[1])\n",
    "            if i in interactions_count:\n",
    "                interactions_count[i] += 1\n",
    "            else:\n",
    "                interactions_count[i] = 1\n",
    "    return interactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_count = create_interactions_count_dict(tweets_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results conveniently through a DataFrame, by first sorting it with descending counts (I only show the top interactions e.g. whose count value are at least 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(data=[(el[0].source, el[0].target, el[1]) for el in interactions_count.items()], columns=[\"Source\", \"Target\", \"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>bibleloussegond</td>\n",
       "      <td>linformatrice</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91363</th>\n",
       "      <td>mumtazceltik</td>\n",
       "      <td>whitehouse</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33120</th>\n",
       "      <td>botcharlie</td>\n",
       "      <td>pressmoustache</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132721</th>\n",
       "      <td>gendarmerie</td>\n",
       "      <td>pnationale</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45210</th>\n",
       "      <td>mumtazceltik</td>\n",
       "      <td>fhollande</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80880</th>\n",
       "      <td>vp</td>\n",
       "      <td>mumtazceltik</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101191</th>\n",
       "      <td>ivorydove</td>\n",
       "      <td>johnkerry</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59980</th>\n",
       "      <td>fhollande</td>\n",
       "      <td>whitehouse</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116031</th>\n",
       "      <td>vp</td>\n",
       "      <td>whitehouse</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138969</th>\n",
       "      <td>mumtazceltik</td>\n",
       "      <td>senjohnmccain</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25879</th>\n",
       "      <td>fhollande</td>\n",
       "      <td>senjohnmccain</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35958</th>\n",
       "      <td>fhollande</td>\n",
       "      <td>matteorenzi</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31528</th>\n",
       "      <td>gwenmetalfly</td>\n",
       "      <td>francetvinfo</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>david_cameron</td>\n",
       "      <td>fhollande</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Source          Target  Count\n",
       "3983    bibleloussegond   linformatrice    569\n",
       "91363      mumtazceltik      whitehouse    282\n",
       "33120        botcharlie  pressmoustache    278\n",
       "132721      gendarmerie      pnationale    190\n",
       "45210      mumtazceltik       fhollande    180\n",
       "80880                vp    mumtazceltik    172\n",
       "101191        ivorydove       johnkerry    162\n",
       "59980         fhollande      whitehouse    144\n",
       "116031               vp      whitehouse    132\n",
       "138969     mumtazceltik   senjohnmccain    124\n",
       "25879         fhollande   senjohnmccain    120\n",
       "35958         fhollande     matteorenzi    107\n",
       "31528      gwenmetalfly    francetvinfo    103\n",
       "7         david_cameron       fhollande    100"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.sort_values(by=\"Count\", ascending=False, inplace=True)\n",
    "interactions_df[interactions_df[\"Count\"]>=100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above DataFrame therefore shows who are the pairs of users that interacted the most on a time basis about hashtag **#JeSuisCharlie**. Please remind that because of the filtering performed in the previous section, DataFrame might include some users that are not part of the graph data.\n",
    "\n",
    "Another perspective may be given instead to highlight which individual users have been involved in the highest number of interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70879</th>\n",
       "      <td>pressmoustache</td>\n",
       "      <td>13229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92740</th>\n",
       "      <td>youtube</td>\n",
       "      <td>4448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16237</th>\n",
       "      <td>charlie_hebdo_</td>\n",
       "      <td>2761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29537</th>\n",
       "      <td>fhollande</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61545</th>\n",
       "      <td>mumtazceltik</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11096</th>\n",
       "      <td>bfmtv</td>\n",
       "      <td>1284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49947</th>\n",
       "      <td>lemondefr</td>\n",
       "      <td>1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90996</th>\n",
       "      <td>whitehouse</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87537</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50684</th>\n",
       "      <td>libe</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12433</th>\n",
       "      <td>botcharlie</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67760</th>\n",
       "      <td>patrickpelloux</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70218</th>\n",
       "      <td>pnationale</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18345</th>\n",
       "      <td>cnn</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78140</th>\n",
       "      <td>senjohnmccain</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90209</th>\n",
       "      <td>vp</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>afpfr</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21241</th>\n",
       "      <td>david_cameron</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51115</th>\n",
       "      <td>linformatrice</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32922</th>\n",
       "      <td>gendarmerie</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54549</th>\n",
       "      <td>manuelvalls</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>bibleloussegond</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58168</th>\n",
       "      <td>metronews</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63328</th>\n",
       "      <td>newyorker</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42873</th>\n",
       "      <td>joachimroncin</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67297</th>\n",
       "      <td>paris</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  User  Count\n",
       "70879   pressmoustache  13229\n",
       "92740          youtube   4448\n",
       "16237   charlie_hebdo_   2761\n",
       "29537        fhollande   2406\n",
       "61545     mumtazceltik   1382\n",
       "11096            bfmtv   1284\n",
       "49947        lemondefr   1240\n",
       "90996       whitehouse   1074\n",
       "87537          twitter   1008\n",
       "50684             libe    974\n",
       "12433       botcharlie    924\n",
       "67760   patrickpelloux    888\n",
       "70218       pnationale    834\n",
       "18345              cnn    688\n",
       "78140    senjohnmccain    638\n",
       "90209               vp    634\n",
       "2731             afpfr    625\n",
       "21241    david_cameron    611\n",
       "51115    linformatrice    610\n",
       "32922      gendarmerie    605\n",
       "54549      manuelvalls    592\n",
       "11198  bibleloussegond    570\n",
       "58168        metronews    561\n",
       "63328        newyorker    548\n",
       "42873    joachimroncin    526\n",
       "67297            paris    500"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = interactions_df.groupby(['Source']).sum().reset_index().rename(columns={'Source':'User'})\n",
    "temp2 = interactions_df.groupby(['Target']).sum().reset_index().rename(columns={'Target':'User'})\n",
    "temp = temp1.append(temp2)\n",
    "temp = temp.groupby(['User']).sum().reset_index()\n",
    "temp.sort_values(by=\"Count\",ascending=False, inplace=True)\n",
    "temp[temp[\"Count\"]>=500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above gives an idea about who have been the most active users around the hashtag topic overtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measuring interactions consistency overtime\n",
    "What I'm now interested in, and what actually represents the end goal of this whole work, is identifying a measure of consistency related to the interactions of people taking part of controversies on Twitter. According to the data I am provided with, I may identify 2 paths:\n",
    "- **MMR graph data**: there's a property of the graph data that hasn't been used so far but still lies in the CSV files I got as result of all the data transformation steps that featured the first notebooks. This property would already give a built-in definition of *interaction consistency*; however, the information we may infer from it is quite limited, and the reason is that for each pair of users we only know whether they have interacted (consistently) within a 3 months period, and given the ranges of our timeline this sums up to a total of 13 periods. It's the only way, however, to compare how interaction consistency has changed overtime;\n",
    "- **Tweets**: when limiting the scope to the tweets related to a specific topic (hashtag), I have no information about which other topics all involved users have been tweeting about neither the related temporal information. Therefore, I may only know when has the *first* Twitter interaction occurred between each pair of mutually interacting users and all the subsequent ones. This is something that has already been highlighted a bit with the previous section, however I may further narrow it down and measure interactions consistency overtime if I count the interactions *per month*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Long-term interaction consistency comparison: Analyzing behavioral changes overtime\n",
    "The idea of this analysis is to compare interaction consistency before and after the first $H$-type interaction, in order to highlight the bonding effect of a given hashtag: does it actually strengthen or weaken ties overtime? Are users more or less likely to communicate with each other after their $H$ interaction? We want to address these questions by carrying out this first timeline analysis by using metadata as edge attributes provided by the MMR graph data.\n",
    "\n",
    "In details: we have a total of 13 periods, ranging from 2013-09 to 2016-12, each of 3 months duration. For each edge in $H$ and for each of the periods, we then have a binary variable with values 1/0, set to 1 if the interaction occurred in the respective 3 months period, 0 otherwise (simplification of True/False values). Let's say, given period with 0-based index $i$ and edge $e$: $IsInPeriod(e,i)$ is a function as defined below:\n",
    "\n",
    "$$\n",
    "IsInPeriod(e,i) =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if $e$ occurred in period $i$,}\\\\\n",
    "        0 & \\text{otherwise.}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Then, the **average mutual interaction consistency** $\\langle MIC \\rangle$ of edge $e$, calculated over $P$ consecutive periods starting from period with index $i$ until period with index $j$, is given by:\n",
    "\n",
    "$$\\langle MIC \\rangle = \\sum_{i=i}^{j}\\frac{IsInPeriod(e,i)}{P}$$\n",
    "\n",
    "Note that $P = j-i+1$.\n",
    "\n",
    "$\\langle MIC \\rangle$ provides a metric I may calculate as:\n",
    "- $\\langle MIC \\rangle_{T}$, where $T$ stands for *total*, and $P = 13$ (the whole timeline period);\n",
    "- $\\langle MIC \\rangle_{B}$ ($B$ = *before*), $P$ is the number of consecutive periods *before* the first $H$ interaction;\n",
    "- $\\langle MIC \\rangle_{A}$ ($A$ = *after*), $P$ is the number of consecutive periods *after* the first $H$ interaction.\n",
    "\n",
    "$\\langle MIC \\rangle_{B}$ and $\\langle MIC \\rangle_{A}$ may be then directly compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we need to conveniently store the edges attribute. This metadata is already transformed in a convenient, light type that assumes periods have been encoded, such that their indexes range from 0 to 12, where period $0$ is period between 2013-09 and 2013-11 (edges included),  and period $12$ between 2016-09 and 2016-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict with Interaction as keys and list of periods as values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Detailed Interaction consistency: how long do ties survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
